{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Inference & GenAI Demo\n",
        "Demonstrating the trained Survival Prediction model and the Generative AI model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from src.models.model_survival import SurvivalCNN\n",
        "from src.models.model_gen import UNetGenerator\n",
        "from src.data.dataset import TCGAPatchDataset\n",
        "\n",
        "DEVICE = torch.device(\"cpu\") # Use CPU for demo inference\n",
        "\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Survival Prediction (Phase 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Model\n",
        "model_surv = SurvivalCNN().to(DEVICE)\n",
        "ckpt_path = Path(\"../checkpoints/survival/best_model.pth\")\n",
        "\n",
        "if ckpt_path.exists():\n",
        "    model_surv.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    model_surv.eval()\n",
        "    print(\"Survival Model Loaded.\")\n",
        "else:\n",
        "    print(\"Checkpoint not found. Run pipeline first.\")\n",
        "\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Inference on a Sample\n",
        "ds = TCGAPatchDataset(split=\"val\", patch_dir=\"../data/processed/patches\", \n",
        "                      clinical_file=\"../data/processed/clinical_processed.csv\",\n",
        "                      manifest_file=\"../data/raw/image_manifest.csv\")\n",
        "\n",
        "if len(ds) > 0:\n",
        "    img, clinical, target = ds[0]\n",
        "    img_tensor = img.unsqueeze(0).to(DEVICE) # Add batch dim\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        risk_score = model_surv(img_tensor)\n",
        "        \n",
        "    print(f\"Predicted Risk Score: {risk_score.item():.4f}\")\n",
        "    print(f\"Actual Time: {target[0]:.1f} months, Event: {target[1]}\")\n",
        "    \n",
        "    plt.imshow(img.permute(1, 2, 0)) # CHW -> HWC\n",
        "    plt.title(f\"Risk: {risk_score.item():.2f}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Dataset empty.\")\n",
        "\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Generative AI: Rewinding Disease (Phase 3)\n",
        "Translating Late Stage images to Early Stage appearance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GenAI Model\n",
        "gen = UNetGenerator().to(DEVICE)\n",
        "ckpt_gen_path = Path(\"../checkpoints/gen/G_L2E_epoch15.pth\") # Try to load a later epoch\n",
        "\n",
        "if not ckpt_gen_path.exists():\n",
        "    # Fallback to any saved\n",
        "    avail = list(Path(\"../checkpoints/gen\").glob(\"*.pth\"))\n",
        "    if avail:\n",
        "        ckpt_gen_path = sorted(avail)[-1]\n",
        "\n",
        "if ckpt_gen_path.exists():\n",
        "    gen.load_state_dict(torch.load(ckpt_gen_path, map_location=DEVICE))\n",
        "    gen.eval()\n",
        "    print(f\"GenAI Model Loaded from {ckpt_gen_path.name}\")\n",
        "else:\n",
        "    print(\"GenAI Checkpoint not found.\")\n",
        "\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Translation (more realistic rendering)\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def tensor_to_rgb01(t: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Convert a CHW tensor to RGB in [0,1] for display.\n",
        "    Handles common cases:\n",
        "      - generator outputs tanh: [-1,1]\n",
        "      - model/data in [0,1]\n",
        "      - ImageNet-normalized-ish tensors\n",
        "    \"\"\"\n",
        "    x = t.detach().float().cpu()\n",
        "    if x.dim() == 4:\n",
        "        x = x[0]\n",
        "    x = x.clamp(-10, 10)\n",
        "\n",
        "    # Heuristic 1: tanh output\n",
        "    if x.min() < -0.2 and x.max() <= 1.2:\n",
        "        x = (x + 1.0) / 2.0\n",
        "        return x.clamp(0, 1)\n",
        "\n",
        "    # Heuristic 2: already [0,1]\n",
        "    if x.min() >= 0.0 and x.max() <= 1.2:\n",
        "        return x.clamp(0, 1)\n",
        "\n",
        "    # Heuristic 3: looks like ImageNet normalization\n",
        "    mean_abs = x.mean().abs().item()\n",
        "    max_abs = x.abs().max().item()\n",
        "    if mean_abs < 0.6 and max_abs < 6.0:\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        x = x * std + mean\n",
        "        return x.clamp(0, 1)\n",
        "\n",
        "    # Fallback: min-max scale per-tensor\n",
        "    x = (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
        "    return x.clamp(0, 1)\n",
        "\n",
        "def color_transfer_match_mean_std(src_rgb01: torch.Tensor, ref_rgb01: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Match per-channel mean/std of src to ref to improve stain realism.\n",
        "    Both are CHW in [0,1].\n",
        "    \"\"\"\n",
        "    src = src_rgb01.clone()\n",
        "    ref = ref_rgb01\n",
        "    for c in range(3):\n",
        "        s = src[c]\n",
        "        r = ref[c]\n",
        "        s_mean, s_std = s.mean(), s.std().clamp(min=1e-6)\n",
        "        r_mean, r_std = r.mean(), r.std().clamp(min=1e-6)\n",
        "        src[c] = (s - s_mean) / s_std * r_std + r_mean\n",
        "    return src.clamp(0, 1)\n",
        "\n",
        "def contrast_stretch(rgb01: torch.Tensor, lo=0.01, hi=0.99) -> torch.Tensor:\n",
        "    \"\"\"Percentile-based contrast stretch to reduce haze.\"\"\"\n",
        "    x = rgb01.clone()\n",
        "    flat = x.view(3, -1)\n",
        "    lo_v = torch.quantile(flat, lo, dim=1).view(3, 1, 1)\n",
        "    hi_v = torch.quantile(flat, hi, dim=1).view(3, 1, 1)\n",
        "    x = (x - lo_v) / (hi_v - lo_v + 1e-6)\n",
        "    return x.clamp(0, 1)\n",
        "\n",
        "if len(ds) > 0:\n",
        "    # Pick a random sample\n",
        "    idx = np.random.randint(0, len(ds))\n",
        "    real_img, _, _ = ds[idx]\n",
        "    real_tensor = real_img.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        fake_early = gen(real_tensor)\n",
        "\n",
        "    # Convert both to display space robustly\n",
        "    real_rgb = tensor_to_rgb01(real_img)\n",
        "    fake_rgb = tensor_to_rgb01(fake_early)\n",
        "\n",
        "    # Improve realism: match stain tone/statistics to the input patch\n",
        "    fake_rgb = color_transfer_match_mean_std(fake_rgb, real_rgb)\n",
        "\n",
        "    # Light contrast improvement\n",
        "    real_disp = contrast_stretch(real_rgb)\n",
        "    fake_disp = contrast_stretch(fake_rgb)\n",
        "\n",
        "    # Plot Side by Side\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    ax[0].imshow(real_disp.permute(1, 2, 0))\n",
        "    ax[0].set_title(\"Input (Original)\")\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(fake_disp.permute(1, 2, 0))\n",
        "    ax[1].set_title(\"Generated (Early Stage) \u2013 corrected display\")\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    plt.savefig(\"latest_inference.png\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Dataset empty.\")\n"
      ],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}